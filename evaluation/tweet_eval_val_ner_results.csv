,model_name,task,dataset,cost,LOC_f1,LOC_number,LOC_precision,LOC_recall,MISC_f1,MISC_number,MISC_precision,MISC_recall,ORG_f1,ORG_number,ORG_precision,ORG_recall,PER_f1,PER_number,PER_precision,PER_recall,overall_accuracy,overall_f1,overall_precision,overall_recall,DATE_f1,DATE_number,DATE_precision,DATE_recall
0,Babelscape/wikineural-multilingual-ner,ner,tweet_eval,0.148,0.397,1151.0,0.265,0.785,0.078,3124.0,0.042,0.635,0.268,3429.0,0.309,0.237,0.688,4508.0,0.75,0.636,0.435,0.188,0.114,0.538,,,,
1,brandon25/deberta-base-finetuned-ner,ner,tweet_eval,0.267,0.233,1151.0,0.229,0.237,0.136,3124.0,0.152,0.122,0.156,3429.0,0.229,0.119,0.582,4508.0,0.652,0.525,0.861,0.322,0.377,0.281,,,,
2,Davlan/bert-base-multilingual-cased-ner-hrl,ner,tweet_eval,0.149,0.399,1151.0,0.273,0.745,0.0,3124.0,0.0,0.0,0.266,3429.0,0.291,0.246,0.677,4508.0,0.638,0.72,0.883,0.424,0.444,0.405,,,,
3,Davlan/distilbert-base-multilingual-cased-ner-hrl,ner,tweet_eval,0.082,0.415,1151.0,0.289,0.735,0.0,3124.0,0.0,0.0,0.303,3429.0,0.319,0.289,0.658,4508.0,0.655,0.661,0.887,0.423,0.455,0.394,,,,
4,Davlan/xlm-roberta-base-ner-hrl,ner,tweet_eval,0.141,0.464,1151.0,0.342,0.718,0.0,3124.0,0.0,0.0,0.285,3429.0,0.33,0.251,0.727,4508.0,0.666,0.8,0.893,0.468,0.507,0.434,,,,
5,flair/ner-english,ner,tweet_eval,0.034,0.701,1151.0,0.721,0.682,0.387,3124.0,0.327,0.473,0.471,3429.0,0.514,0.434,0.785,4508.0,0.745,0.829,0.905,0.582,0.554,0.613,,,,
6,flair/ner-english-fast,ner,tweet_eval,0.017,0.645,1151.0,0.621,0.672,0.378,3124.0,0.386,0.371,0.468,3429.0,0.494,0.444,0.762,4508.0,0.718,0.811,0.906,0.577,0.573,0.582,,,,
7,flair/ner-multi-fast,ner,tweet_eval,0.017,0.747,1151.0,0.786,0.712,0.499,3124.0,0.466,0.537,0.629,3429.0,0.651,0.608,0.862,4508.0,0.841,0.884,0.935,0.691,0.681,0.702,,,,
8,gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner,ner,tweet_eval,0.079,0.437,1151.0,0.307,0.754,0.261,3124.0,0.53,0.173,0.318,3429.0,0.283,0.364,0.698,4508.0,0.664,0.736,0.884,0.469,0.451,0.489,,,,
9,huggingface-course/bert-finetuned-ner,ner,tweet_eval,0.139,0.448,1151.0,0.335,0.674,0.286,3124.0,0.428,0.214,0.287,3429.0,0.287,0.288,0.732,4508.0,0.693,0.775,0.891,0.482,0.48,0.485,,,,
10,Jean-Baptiste/camembert-ner-with-dates,ner,tweet_eval,0.144,0.212,1151.0,0.131,0.567,0.099,3124.0,0.056,0.453,0.18,3429.0,0.135,0.269,0.342,4508.0,0.218,0.791,0.468,0.198,0.121,0.537,0.0,0.0,0.0,0.0
11,kamalkraj/bert-base-cased-ner-conll2003,ner,tweet_eval,0.139,0.475,1151.0,0.365,0.683,0.302,3124.0,0.434,0.231,0.292,3429.0,0.259,0.334,0.725,4508.0,0.718,0.733,0.888,0.476,0.464,0.488,,,,
12,elastic/distilbert-base-uncased-finetuned-conll03-english,ner,tweet_eval,0.082,0.375,1151.0,0.257,0.695,0.289,3124.0,0.475,0.208,0.264,3429.0,0.189,0.438,0.668,4508.0,0.63,0.712,0.843,0.414,0.351,0.504,,,,
13,elastic/distilbert-base-cased-finetuned-conll03-english,ner,tweet_eval,0.082,0.428,1151.0,0.3,0.744,0.303,3124.0,0.562,0.207,0.298,3429.0,0.264,0.341,0.698,4508.0,0.654,0.749,0.885,0.469,0.445,0.495,,,,
14,dominiqueblok/roberta-base-finetuned-ner,ner,tweet_eval,0.147,0.58,1151.0,0.514,0.666,0.284,3124.0,0.562,0.19,0.336,3429.0,0.291,0.397,0.746,4508.0,0.718,0.777,0.068,0.12,0.068,0.509,0.0,0.0,0.0,0.0
15,bhadresh-savani/electra-base-discriminator-finetuned-conll03-english,ner,tweet_eval,0.144,0.33,1151.0,0.216,0.694,0.287,3124.0,0.389,0.228,0.184,3429.0,0.118,0.415,0.662,4508.0,0.58,0.77,0.782,0.359,0.272,0.524,,,,
16,malduwais/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.081,0.474,1151.0,0.395,0.591,0.252,3124.0,0.403,0.184,0.291,3429.0,0.247,0.355,0.692,4508.0,0.697,0.687,0.061,0.108,0.061,0.456,0.0,0.0,0.0,0.0
17,sberbank-ai/bert-base-NER-reptile-5-datasets,ner,tweet_eval,0.145,0.0,1151.0,0.0,0.0,0.0,3124.0,0.0,0.0,0.0,3429.0,0.0,0.0,0.002,4508.0,0.017,0.001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
18,kamalkraj/bert-base-cased-ner-conll2003,ner,tweet_eval,0.146,0.475,1151.0,0.365,0.683,0.302,3124.0,0.434,0.231,0.292,3429.0,0.259,0.334,0.725,4508.0,0.718,0.733,0.888,0.476,0.464,0.488,,,,
19,autoevaluate/entity-extraction,ner,tweet_eval,0.081,0.438,1151.0,0.342,0.609,0.236,3124.0,0.512,0.153,0.236,3429.0,0.172,0.379,0.614,4508.0,0.604,0.624,0.852,0.386,0.348,0.433,,,,
20,ArBert/albert-base-v2-finetuned-ner,ner,tweet_eval,0.161,0.552,1151.0,0.53,0.575,0.188,3124.0,0.719,0.108,0.22,3429.0,0.275,0.183,0.723,4508.0,0.675,0.778,0.056,0.099,0.056,0.42,0.0,0.0,0.0,0.0
21,roschmid/distilbert-base-uncased-finetuned-TT2-exam,ner,tweet_eval,0.083,0.488,1151.0,0.429,0.566,0.253,3124.0,0.433,0.178,0.274,3429.0,0.208,0.404,0.0,0.0,0.0,0.0,0.637,4508.0,0.688,0.593,0.058,0.102,0.058,0.431
22,murdockthedude/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.083,0.48,1151.0,0.459,0.503,0.226,3124.0,0.507,0.145,0.285,3429.0,0.218,0.41,0.0,0.0,0.0,0.0,0.624,4508.0,0.718,0.552,0.054,0.095,0.054,0.403
23,kushaljoseph/bert-to-distilbert-NER,ner,tweet_eval,0.081,0.02,1151.0,0.021,0.019,0.097,3124.0,0.084,0.117,0.099,3429.0,0.126,0.082,,,,,0.144,4508.0,0.125,0.169,0.811,0.11,0.104,0.117
24,Nonzerophilip/bert-finetuned-ner,ner,tweet_eval,0.147,0.421,1151.0,0.32,0.617,0.114,3124.0,0.252,0.073,0.247,3429.0,0.196,0.334,,,,,0.712,4508.0,0.696,0.729,0.869,0.415,0.392,0.44
25,romainlhardy/bert-finetuned-ner,ner,tweet_eval,0.147,0.492,1151.0,0.389,0.668,0.287,3124.0,0.376,0.232,0.288,3429.0,0.262,0.319,,,,,0.735,4508.0,0.684,0.795,0.888,0.483,0.463,0.505
26,hossay/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.082,0.43,1151.0,0.323,0.639,0.25,3124.0,0.389,0.184,0.233,3429.0,0.16,0.43,,,,,0.655,4508.0,0.595,0.728,0.833,0.395,0.328,0.497
27,jjglilleberg/bert-finetuned-ner,ner,tweet_eval,0.146,0.456,1151.0,0.339,0.694,0.286,3124.0,0.429,0.214,0.285,3429.0,0.252,0.329,,,,,0.721,4508.0,0.706,0.737,0.886,0.468,0.452,0.485
28,MikhailGalperin/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.083,0.017,1151.0,0.009,0.251,0.073,3124.0,0.041,0.33,0.023,3429.0,0.082,0.013,0.0,0.0,0.0,0.0,0.111,4508.0,0.064,0.438,0.037,0.065,0.037,0.274
29,Aneela/bert-finetuned-ner,ner,tweet_eval,0.146,0.474,1151.0,0.372,0.656,0.29,3124.0,0.406,0.225,0.322,3429.0,0.285,0.37,,,,,0.737,4508.0,0.695,0.784,0.89,0.491,0.471,0.513
