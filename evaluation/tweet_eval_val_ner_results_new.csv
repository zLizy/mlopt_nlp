,model_name,task,dataset,cost,LOC_f1,LOC_number,LOC_precision,LOC_recall,MISC_f1,MISC_number,MISC_precision,MISC_recall,ORG_f1,ORG_number,ORG_precision,ORG_recall,PER_f1,PER_number,PER_precision,PER_recall,overall_accuracy,overall_f1,overall_precision,overall_recall,DATE_f1,DATE_number,DATE_precision,DATE_recall
0,Babelscape/wikineural-multilingual-ner,ner,tweet_eval,0.148,0.397,1151.0,0.265,0.785,0.078,3124.0,0.042,0.635,0.268,3429.0,0.309,0.237,0.688,4508.0,0.75,0.636,0.435,0.188,0.114,0.538,,,,
1,brandon25/deberta-base-finetuned-ner,ner,tweet_eval,0.267,0.233,1151.0,0.229,0.237,0.136,3124.0,0.152,0.122,0.156,3429.0,0.229,0.119,0.582,4508.0,0.652,0.525,0.861,0.322,0.377,0.281,,,,
2,Davlan/bert-base-multilingual-cased-ner-hrl,ner,tweet_eval,0.149,0.399,1151.0,0.273,0.745,0.0,3124.0,0.0,0.0,0.266,3429.0,0.291,0.246,0.677,4508.0,0.638,0.72,0.883,0.424,0.444,0.405,,,,
3,Davlan/distilbert-base-multilingual-cased-ner-hrl,ner,tweet_eval,0.082,0.415,1151.0,0.289,0.735,0.0,3124.0,0.0,0.0,0.303,3429.0,0.319,0.289,0.658,4508.0,0.655,0.661,0.887,0.423,0.455,0.394,,,,
4,Davlan/xlm-roberta-base-ner-hrl,ner,tweet_eval,0.141,0.464,1151.0,0.342,0.718,0.0,3124.0,0.0,0.0,0.285,3429.0,0.33,0.251,0.727,4508.0,0.666,0.8,0.893,0.468,0.507,0.434,,,,
5,flair/ner-english,ner,tweet_eval,0.034,0.701,1151.0,0.721,0.682,0.387,3124.0,0.327,0.473,0.471,3429.0,0.514,0.434,0.785,4508.0,0.745,0.829,0.905,0.582,0.554,0.613,,,,
6,flair/ner-english-fast,ner,tweet_eval,0.017,0.645,1151.0,0.621,0.672,0.378,3124.0,0.386,0.371,0.468,3429.0,0.494,0.444,0.762,4508.0,0.718,0.811,0.906,0.577,0.573,0.582,,,,
7,flair/ner-multi-fast,ner,tweet_eval,0.017,0.747,1151.0,0.786,0.712,0.499,3124.0,0.466,0.537,0.629,3429.0,0.651,0.608,0.862,4508.0,0.841,0.884,0.935,0.691,0.681,0.702,,,,
8,gunghio/distilbert-base-multilingual-cased-finetuned-conll2003-ner,ner,tweet_eval,0.079,0.437,1151.0,0.307,0.754,0.261,3124.0,0.53,0.173,0.318,3429.0,0.283,0.364,0.698,4508.0,0.664,0.736,0.884,0.469,0.451,0.489,,,,
9,huggingface-course/bert-finetuned-ner,ner,tweet_eval,0.139,0.448,1151.0,0.335,0.674,0.286,3124.0,0.428,0.214,0.287,3429.0,0.287,0.288,0.732,4508.0,0.693,0.775,0.891,0.482,0.48,0.485,,,,
10,Jean-Baptiste/camembert-ner-with-dates,ner,tweet_eval,0.144,0.212,1151.0,0.131,0.567,0.099,3124.0,0.056,0.453,0.18,3429.0,0.135,0.269,0.342,4508.0,0.218,0.791,0.468,0.198,0.121,0.537,0.0,0.0,0.0,0.0
11,kamalkraj/bert-base-cased-ner-conll2003,ner,tweet_eval,0.139,0.475,1151.0,0.365,0.683,0.302,3124.0,0.434,0.231,0.292,3429.0,0.259,0.334,0.725,4508.0,0.718,0.733,0.888,0.476,0.464,0.488,,,,
12,elastic/distilbert-base-uncased-finetuned-conll03-english,ner,tweet_eval,0.082,0.375,1151.0,0.257,0.695,0.289,3124.0,0.475,0.208,0.264,3429.0,0.189,0.438,0.668,4508.0,0.63,0.712,0.843,0.414,0.351,0.504,,,,
13,elastic/distilbert-base-cased-finetuned-conll03-english,ner,tweet_eval,0.082,0.428,1151.0,0.3,0.744,0.303,3124.0,0.562,0.207,0.298,3429.0,0.264,0.341,0.698,4508.0,0.654,0.749,0.885,0.469,0.445,0.495,,,,
14,dominiqueblok/roberta-base-finetuned-ner,ner,tweet_eval,0.147,0.58,1151.0,0.514,0.666,0.284,3124.0,0.562,0.19,0.336,3429.0,0.291,0.397,0.746,4508.0,0.718,0.777,0.068,0.12,0.068,0.509,0.0,0.0,0.0,0.0
15,bhadresh-savani/electra-base-discriminator-finetuned-conll03-english,ner,tweet_eval,0.144,0.33,1151.0,0.216,0.694,0.287,3124.0,0.389,0.228,0.184,3429.0,0.118,0.415,0.662,4508.0,0.58,0.77,0.782,0.359,0.272,0.524,,,,
16,malduwais/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.081,0.474,1151.0,0.395,0.591,0.252,3124.0,0.403,0.184,0.291,3429.0,0.247,0.355,0.692,4508.0,0.697,0.687,0.061,0.108,0.061,0.456,0.0,0.0,0.0,0.0
17,sberbank-ai/bert-base-NER-reptile-5-datasets,ner,tweet_eval,0.145,0.0,1151.0,0.0,0.0,0.0,3124.0,0.0,0.0,0.0,3429.0,0.0,0.0,0.002,4508.0,0.017,0.001,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0
18,kamalkraj/bert-base-cased-ner-conll2003,ner,tweet_eval,0.146,0.475,1151.0,0.365,0.683,0.302,3124.0,0.434,0.231,0.292,3429.0,0.259,0.334,0.725,4508.0,0.718,0.733,0.888,0.476,0.464,0.488,,,,
19,autoevaluate/entity-extraction,ner,tweet_eval,0.081,0.438,1151.0,0.342,0.609,0.236,3124.0,0.512,0.153,0.236,3429.0,0.172,0.379,0.614,4508.0,0.604,0.624,0.852,0.386,0.348,0.433,,,,
20,ArBert/albert-base-v2-finetuned-ner,ner,tweet_eval,0.161,0.552,1151.0,0.53,0.575,0.188,3124.0,0.719,0.108,0.22,3429.0,0.275,0.183,0.723,4508.0,0.675,0.778,0.056,0.099,0.056,0.42,0.0,0.0,0.0,0.0
21,roschmid/distilbert-base-uncased-finetuned-TT2-exam,ner,tweet_eval,0.083,0.488,1151.0,0.429,0.566,0.253,3124.0,0.433,0.178,0.274,3429.0,0.208,0.404,0.0,0.0,0.0,0.0,0.637,4508.0,0.688,0.593,0.058,0.102,0.058,0.431
22,murdockthedude/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.083,0.48,1151.0,0.459,0.503,0.226,3124.0,0.507,0.145,0.285,3429.0,0.218,0.41,0.0,0.0,0.0,0.0,0.624,4508.0,0.718,0.552,0.054,0.095,0.054,0.403
23,kushaljoseph/bert-to-distilbert-NER,ner,tweet_eval,0.081,0.02,1151.0,0.021,0.019,0.097,3124.0,0.084,0.117,0.099,3429.0,0.126,0.082,,,,,0.144,4508.0,0.125,0.169,0.811,0.11,0.104,0.117
24,Nonzerophilip/bert-finetuned-ner,ner,tweet_eval,0.147,0.421,1151.0,0.32,0.617,0.114,3124.0,0.252,0.073,0.247,3429.0,0.196,0.334,,,,,0.712,4508.0,0.696,0.729,0.869,0.415,0.392,0.44
25,romainlhardy/bert-finetuned-ner,ner,tweet_eval,0.147,0.492,1151.0,0.389,0.668,0.287,3124.0,0.376,0.232,0.288,3429.0,0.262,0.319,,,,,0.735,4508.0,0.684,0.795,0.888,0.483,0.463,0.505
26,hossay/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.082,0.43,1151.0,0.323,0.639,0.25,3124.0,0.389,0.184,0.233,3429.0,0.16,0.43,,,,,0.655,4508.0,0.595,0.728,0.833,0.395,0.328,0.497
27,jjglilleberg/bert-finetuned-ner,ner,tweet_eval,0.146,0.456,1151.0,0.339,0.694,0.286,3124.0,0.429,0.214,0.285,3429.0,0.252,0.329,,,,,0.721,4508.0,0.706,0.737,0.886,0.468,0.452,0.485
28,MikhailGalperin/distilbert-base-uncased-finetuned-ner,ner,tweet_eval,0.083,0.017,1151.0,0.009,0.251,0.073,3124.0,0.041,0.33,0.023,3429.0,0.082,0.013,0.0,0.0,0.0,0.0,0.111,4508.0,0.064,0.438,0.037,0.065,0.037,0.274
29,Aneela/bert-finetuned-ner,ner,tweet_eval,0.146,0.474,1151.0,0.372,0.656,0.29,3124.0,0.406,0.225,0.322,3429.0,0.285,0.37,,,,,0.737,4508.0,0.695,0.784,0.89,0.491,0.471,0.513
30,Davlan-bert-base-multilingual-cased-ner-hrl-finetuned-tweet_eval,ner,tweet_eval,0.013291462480778029,0.718677869079494,4605.0,0.625120462576293,0.8451682953311618,0.6088825214899714,21532.0,0.5572265926268702,0.6710941853984767,0.6953994743477425,26058.0,0.7845330504797261,0.6244531429887175,0.8573226647487189,15527.0,0.8017038448604417,0.9212339795195466,0.9280823164737324,0.7065858749268757,0.6915084604401973,0.72233543014087,,,,
31,Davlan-distilbert-base-multilingual-cased-ner-hrl-finetuned-tweet_eval,ner,tweet_eval,0.007348339157728756,0.7688392165541663,4605.0,0.7838447653429603,0.7543973941368078,0.6312992486714311,21532.0,0.6228530103055505,0.6399777075979937,0.7363707091216152,26058.0,0.72831350174543,0.7446081817484074,0.8813104703515663,15527.0,0.8739708676377455,0.8887743929928512,0.9350876639375102,0.7381664703860025,0.7313986693145085,0.7450606892885621,,,,
32,gunghio-distilbert-base-multilingual-cased-finetuned-conll2003-ner-finetuned-tweet_eval,ner,tweet_eval,0.007140306336778009,0.7531685019738209,4605.0,0.7219677355108544,0.7871878393051032,0.6138057360800944,21532.0,0.6675945636155232,0.5680382686234442,0.7215663482146101,26058.0,0.6849655172413793,0.7622994857625297,0.8666747323210937,15527.0,0.8170516110635871,0.9227152701745347,0.9324539834114537,0.7274326121556116,0.7162156360461954,0.7390065266826142,,,,
33,philschmid-distilroberta-base-ner-conll2003-finetuned-tweet_eval,ner,tweet_eval,0.0064106908616512435,0.7678389686757887,4605.0,0.8010854176498349,0.7372421281216069,0.6325436720637653,18822.0,0.6626709339540297,0.6050366592285623,0.7545071043297584,22984.0,0.7288245550014192,0.7820657848938392,0.8853168469860896,15527.0,0.8512155976936338,0.9222644425838862,0.9385889388931289,0.75417110543305,0.7483507924144399,0.7600826633084697,,,,
34,elastic-distilbert-base-uncased-finetuned-conll03-english-finetuned-tweet_eval,ner,tweet_eval,0.0071599519719581445,0.7178286322511974,4605.0,0.7039665970772443,0.7322475570032573,0.5414306342330335,19894.0,0.6048517185755057,0.4900472504272645,0.6817555434918778,23951.0,0.6818694399198095,0.6816416851070937,0.8348621057443243,15527.0,0.7923415085608515,0.8822051909576866,0.9142183566153596,0.6842113609692664,0.694330452694765,0.6743829813839348,,,,
35,elastic-distilbert-base-cased-finetuned-conll03-english-finetuned-tweet_eval,ner,tweet_eval,0.00717199789300291,0.7658456486042693,4605.0,0.7721854304635761,0.7596091205211727,0.6227192738723719,22602.0,0.6532109200427475,0.5949473497920538,0.745185832968955,28714.0,0.7484979445557078,0.7419029045065125,0.8787764350453171,15527.0,0.8592528771001292,0.899207831519289,0.9317867430871978,0.7391310502994182,0.7477157505800132,0.7307412383831597,,,,
36,malduwais-distilbert-base-uncased-finetuned-ner-finetuned-tweet_eval,ner,tweet_eval,0.0071033705472249445,0.7087527352297593,4605.0,0.7142227122381477,0.7033659066232356,0.5549484233814004,19894.0,0.5907053282642002,0.5232733487483664,0.6857424132174806,23951.0,0.7034467618002196,0.6689073525113775,0.848357746660644,15527.0,0.8500032326889506,0.8467186191794938,0.9163711017592129,0.688503501451233,0.7088907284768212,0.6692561389249262,,,,
37,autoevaluate-entity-extraction-finetuned-tweet_eval,ner,tweet_eval,0.007086891238972292,0.7182450508293204,4605.0,0.7080168776371308,0.7287730727470141,0.5524815100729635,19894.0,0.5474510190988501,0.5576053081331055,0.6722395102406085,23951.0,0.655377538071066,0.6899920671370715,0.8417867162626484,15527.0,0.8244010866880711,0.8599214271913441,0.9109971147964474,0.6799036750720903,0.6674245275916585,0.6928583709770699,,,,
38,roschmid-distilbert-base-uncased-finetuned-TT2-exam-finetuned-tweet_eval,ner,tweet_eval,0.007152107290048337,0.7079758500158883,4605.0,0.6910669975186104,0.7257328990228012,0.5447378793851005,19894.0,0.5707284841143109,0.5210113602091083,0.6787748663570239,23951.0,0.7141013230074218,0.646778840131936,0.8360886838395488,15527.0,0.8318775900541919,0.8403426289688929,0.9139558267197677,0.6794583118888318,0.6997267080745342,0.6603310564734202,,,,
39,murdockthedude-distilbert-base-uncased-finetuned-ner-finetuned-tweet_eval,ner,tweet_eval,0.007165040975422416,0.7184088206680359,4605.0,0.7152389151958675,0.7216069489685125,0.5478513196075722,19894.0,0.6084341047203977,0.49824067558057705,0.6826968200406713,23951.0,0.7344456197711319,0.63776042753956,0.8424678985280302,15527.0,0.8199719563494483,0.8662330134604238,0.9173214599812554,0.6872210621135816,0.7217310536998177,0.6558606999390406,,,,
40,kushaljoseph-bert-to-distilbert-NER-finetuned-tweet_eval,ner,tweet_eval,0.0070700276183636125,0.6982207233399232,4605.0,0.7516274411617426,0.651900108577633,0.5557455066303192,22602.0,0.5815122800232063,0.5321652951066277,0.6822722329122396,28714.0,0.6565972110399021,0.7100369157902069,0.8420211575953209,15527.0,0.8357441948991244,0.8483931216590456,0.9143268663450526,0.6798844262237738,0.6796799597152089,0.6800890157877058,,,,
41,hossay-distilbert-base-uncased-finetuned-ner-finetuned-tweet_eval,ner,tweet_eval,0.007152784783404031,0.7044644728568433,4605.0,0.680777800283573,0.7298588490770901,0.5503661513425548,19894.0,0.5976672950047125,0.510003015984719,0.6789184364794418,23951.0,0.6602098784913997,0.6987182163583984,0.8463080202335125,15527.0,0.8362253237771909,0.8566368261737618,0.9141185952550347,0.6849399106524885,0.6893314547152786,0.6806039670506588,,,,
42,jjglilleberg-bert-finetuned-ner-finetuned-tweet_eval,ner,tweet_eval,0.013345545178474146,0.0,4605.0,0.0,0.0,0.0,22602.0,0.0,0.0,0.0,28714.0,0.0,0.0,0.0,15527.0,0.0,0.0,0.8228775701444289,0.0,0.0,0.0,,,,
43,MikhailGalperin-distilbert-base-uncased-finetuned-ner-finetuned-tweet_eval,ner,tweet_eval,0.007149589612148158,0.7137718992922611,4605.0,0.7663178873941205,0.6679695982627579,0.5521532400489064,19894.0,0.6008515167642363,0.5107570121644717,0.6813850698531863,23951.0,0.7279328018223234,0.6404325497891529,0.8402510889059698,15527.0,0.8357010893801363,0.8448509048753784,0.9166047533662895,0.6853564119634097,0.7226748015391549,0.6517029557497226,,,,
